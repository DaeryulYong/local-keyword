{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJNsS6Vumh6gaCWzcmrMlc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Colab í™˜ê²½ì—ì„œ ë‚˜ëˆ” í°íŠ¸ ì„¤ì¹˜\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf\n","\n","# ëŸ°íƒ€ì„ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.\n","print(\"âœ… ë‚˜ëˆ” í°íŠ¸ ì„¤ì¹˜ ì™„ë£Œ. **ëŸ°íƒ€ì„(Runtime)ì„ ì¬ì‹œì‘(Restart Runtime)í•˜ì„¸ìš”.**\")"],"metadata":{"collapsed":true,"id":"l6L-zAae2C9_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„, ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ Matplotlib ì„¤ì •\n","import matplotlib.pyplot as plt\n","\n","# Matplotlibì˜ í°íŠ¸ ì„¤ì •ì„ ë‚˜ëˆ”ê³ ë”•ìœ¼ë¡œ ë³€ê²½\n","plt.rc('font', family='NanumGothic')\n","\n","# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€\n","plt.rcParams['axes.unicode_minus'] = False\n","\n","print(\"âœ… Matplotlib í°íŠ¸ ì„¤ì • ì™„ë£Œ. ì´ì œ í•œê¸€ì´ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\")\n","\n","# âš ï¸ ì´ ì½”ë“œ ì´í›„ì— ì‹œê°í™” ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤."],"metadata":{"id":"Uq6NCum23T8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoNLPy ì„¤ì¹˜\n","!pip install konlpy\n","# JAVA í™˜ê²½ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Colabì—ì„œëŠ” ë³´í†µ ê¸°ë³¸ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤."],"metadata":{"id":"W2iYdZsAP47I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ìƒˆ ì…€ì— ì…ë ¥)\n","!pip install scikit-learn pandas numpy"],"metadata":{"id":"rCd5ox5Myl6z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0e120a39"},"source":["## Initialize KoNLPy and Define Helper Functions\n","\n","### Subtask:\n","KoNLPy ì„¤ì¹˜, Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”, 'clean_business_name' (ìƒí˜¸ëª… ì •ì œ), 'filter_franchise' (í”„ëœì°¨ì´ì´ì¦ˆ í•„í„°ë§), 'analyze_morphemes' (í˜•íƒœì†Œ ë¶„ì„), 'remove_stopwords' (ë¶ˆìš©ì–´ ì œê±°) í•¨ìˆ˜ ë° ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸('stopwords')ë¥¼ í†µí•©ëœ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì‚¬ì „ ì¤€ë¹„ ë‹¨ê³„ì…ë‹ˆë‹¤.\n"]},{"cell_type":"code","metadata":{"id":"8ec8ad64"},"source":["import pandas as pd\n","import re\n","from konlpy.tag import Okt\n","from collections import Counter\n","import itertools\n","import glob\n","import os\n","\n","# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n","okt = Okt()\n","\n","def clean_business_name(text):\n","    \"\"\"\n","    ìƒí˜¸ëª… í…ìŠ¤íŠ¸ì—ì„œ ë¶„ì„ì— ë°©í•´ë˜ëŠ” ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë° ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","\n","    text = re.sub(r'\\([^)]*\\)', '', text)\n","    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n","\n","    text = re.sub(r'\\d+í˜¸?ì ', '', text)\n","    text = re.sub(r'\\s\\w+ì \\b', '', text)\n","\n","    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n","\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text\n","\n","def filter_franchise(df_input):\n","    \"\"\"\n","    ì§€ì—­í™”í ê°€ë§¹ì  ë°ì´í„°ì˜ ìˆœìˆ˜ì„±(ì†Œìƒê³µì¸ ì¤‘ì‹¬)ì„ ìœ ì§€í•˜ê¸° ìœ„í•´\n","    ëŒ€ê·œëª¨ ì²´ì¸ì  ìƒí˜¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    franchise_keywords = ['ìŠ¤íƒ€ë²…ìŠ¤', 'GS25', 'CU', 'ì„¸ë¸ì¼ë ˆë¸', 'íŒŒë¦¬ë°”ê²Œëœ¨', 'ë§¥ë„ë‚ ë“œ', 'ë¡¯ë°ë§ˆíŠ¸']\n","\n","    pattern = '|'.join(franchise_keywords)\n","    filtered_df = df_input[~df_input['ìƒí˜¸ëª…'].str.contains(pattern, na=False)].copy()\n","\n","    return filtered_df\n","\n","def analyze_morphemes(text):\n","    \"\"\"\n","    í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","    \"\"\"\n","    if not isinstance(text, str) or not text.strip():\n","        return []\n","\n","    morphemes = okt.nouns(text)\n","    return morphemes\n","\n","# ë¶ˆìš©ì–´(Stopwords) ì •ì˜\n","stopwords = {\n","    'ì ', 'ìƒí˜¸', 'í•˜ìš°ìŠ¤', 'ì§‘', 'ë³¸ì ', 'ì§€ì ', 'ì„¼í„°', 'ìŠ¤í† ì–´', 'ë§ˆì¼“',\n","    'ì•„íŒŒíŠ¸', 'ë¹Œë”©', 'íƒ€ìš´', 'ëŒ€ë¦¬ì ', 'ì‹œí‹°', 'ëœë“œ', 'ì›”ë“œ', 'íŒŒí¬', 'ê°€ë“ ', 'í”„ë¼ì', 'í´ëŸ½',\n","    'ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì‡¼í•‘', 'ëª°', 'ì•„ìš¸ë ›', 'ë”', 'ì•¤', 'ì•¤ë“œ', 'ì•¤', 'ë² ìŠ¤íŠ¸', 'ì„œë¹„ìŠ¤', 'ì „ë¬¸', 'ì œì¼',\n","    'ê·¸ë£¹', 'ì „', 'í›„','ì§€ì—ìŠ¤','ì”¨ìœ ',\n","    'í˜¸', 'ëª…', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ì™€', 'ë“±',\n","    'ê°€í‰', # ë¶„ì„ ëŒ€ìƒ ì§€ì—­ëª…ì„ ë¶ˆìš©ì–´ì— ì¶”ê°€\n","    'íŒŒì£¼', 'ì„±ë‚¨' # ì˜ˆì‹œ, í•„ìš”ì— ë”°ë¼ ì¶”ê°€\n","}\n","\n","def remove_stopwords(morpheme_list):\n","    \"\"\"\n","    í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ì˜ëœ ë¶ˆìš©ì–´ ë° í•œ ê¸€ì ë‹¨ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    filtered_list = [\n","        word for word in morpheme_list\n","        if word not in stopwords and len(word) > 1\n","    ]\n","    return filtered_list\n","\n","print(\"KoNLPy ë° í—¬í¼ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0b33ece9","collapsed":true},"source":["import pandas as pd\n","import re\n","from konlpy.tag import Okt\n","from collections import Counter\n","import itertools\n","import glob\n","import os\n","\n","# ----------------------------------------------------------------------------\n","# 1. Initialize KoNLPy and Define Helper Functions\n","# This block consolidates all necessary functions for data processing.\n","# ----------------------------------------------------------------------------\n","\n","# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n","okt = Okt()\n","\n","def clean_business_name(text):\n","    \"\"\"\n","    ìƒí˜¸ëª… í…ìŠ¤íŠ¸ì—ì„œ ë¶„ì„ì— ë°©í•´ë˜ëŠ” ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë° ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","\n","    # (1) ë²•ì¸/ë‹¨ì²´ëª… ì œê±°: (ì£¼), (ìœ ), [ë³¸ì ] ë“± ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš© ì‚­ì œ\n","    text = re.sub(r'\\([^)]*\\)', '', text)\n","    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n","\n","    # (2) ì§€ì ëª… ì œê±°: '~ì ', '~í˜¸ì ' ë“± (ì§€ì—­ íŠ¹í™” ë¶„ì„ì˜ ìˆœìˆ˜ì„±ì„ ìœ„í•´ ì œê±°)\n","    text = re.sub(r'\\d+í˜¸?ì ', '', text) # 1í˜¸ì , 2í˜¸ì  ë“± ì œê±°\n","    text = re.sub(r'\\s\\w+ì \\b', '', text) # ë„ì–´ì“°ê¸° í›„ 'ã…‡ã…‡ì 'ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ ì œê±°\n","\n","    # (3) íŠ¹ìˆ˜ë¬¸ì ì œê±°: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°\n","    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n","\n","    # (4) ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬: ë‹¤ì¤‘ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¤„ì´ê³  ì–‘ìª½ ê³µë°± ì œê±°\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text\n","\n","def filter_franchise(df_input):\n","    \"\"\"\n","    ì§€ì—­í™”í ê°€ë§¹ì  ë°ì´í„°ì˜ ìˆœìˆ˜ì„±(ì†Œìƒê³µì¸ ì¤‘ì‹¬)ì„ ìœ ì§€í•˜ê¸° ìœ„í•´\n","    ëŒ€ê·œëª¨ ì²´ì¸ì  ìƒí˜¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    # âš ï¸ ì—¬ê¸°ì— ë¶„ì„ì—ì„œ ì œì™¸í•  í”„ëœì°¨ì´ì¦ˆ ë° ëŒ€ê¸°ì—… ì´ë¦„ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    franchise_keywords = [\n","        'ìŠ¤íƒ€ë²…ìŠ¤', 'GS25', 'CU', 'ì„¸ë¸ì¼ë ˆë¸', 'íŒŒë¦¬ë°”ê²Œëœ¨', 'ë§¥ë„ë‚ ë“œ', 'ë¡¯ë°ë§ˆíŠ¸',\n","        'ë²„ê±°í‚¹', 'KFC', 'ì¨ë¸Œì›¨ì´', 'ì´ë§ˆíŠ¸', 'í™ˆí”ŒëŸ¬ìŠ¤', 'ë¡¯ë°ìŠˆí¼', 'ë…¸ë¸Œëœë“œ',\n","        'ë‹¤ì´ì†Œ', 'ì˜¬ë¦¬ë¸Œì˜', 'ë„ë¼ë¸”ë¼', 'ë¡­ìŠ¤', 'ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤', 'ë˜í‚¨ë„ë„ˆì¸ ', 'íŒŒìŠ¤ì¿ ì°Œ',\n","        'ì—”ì œë¦¬ë„ˆìŠ¤', 'íƒì•¤íƒìŠ¤', 'íˆ¬ì¸í”Œë ˆì´ìŠ¤', 'ë¹½ë‹¤ë°©', 'ì´ë””ì•¼', 'ì»´í¬ì¦ˆì»¤í”¼',\n","        'ë©”ê°€ì»¤í”¼', 'ë§˜ìŠ¤í„°ì¹˜', 'êµì´Œì¹˜í‚¨', 'BBQ', 'BHC', 'êµ½ë„¤ì¹˜í‚¨', 'ë„¤ë„¤ì¹˜í‚¨',\n","        'ë„ë¯¸ë…¸í”¼ì', 'í”¼ìí—›', 'ë¯¸ìŠ¤í„°í”¼ì', 'ì£ ìŠ¤ë–¡ë³¶ì´', 'ì‹ ì „ë–¡ë³¶ì´', 'ì—½ê¸°ë–¡ë³¶ì´',\n","        'êµ­ë¯¼ì€í–‰', 'ì‹ í•œì€í–‰', 'ìš°ë¦¬ì€í–‰', 'í•˜ë‚˜ì€í–‰', 'ë†í˜‘', 'ê¸°ì—…ì€í–‰', 'ìš°ì²´êµ­',\n","        'í˜„ëŒ€ìë™ì°¨', 'ê¸°ì•„ìë™ì°¨', 'ë¥´ë…¸ì‚¼ì„±', 'ìŒìš©ìë™ì°¨', 'ì œë„¤ì‹œìŠ¤',\n","        'SKí…”ë ˆì½¤', 'KT', 'LGìœ í”ŒëŸ¬ìŠ¤', 'ì‚¼ì„±ì „ìì„œë¹„ìŠ¤', 'LGì „ìì„œë¹„ìŠ¤',\n","        'LGìƒí™œê±´ê°•', 'ì•„ëª¨ë ˆí¼ì‹œí”½', 'ì½”ì›¨ì´', 'ì²­í˜¸ë‚˜ì´ìŠ¤', 'êµì›', 'ì›…ì§„', 'ëŒ€êµ', 'êµ¬ëª¬',\n","        'ì¿ íŒ¡', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'ë°°ë‹¬ì˜ë¯¼ì¡±', 'ìš”ê¸°ìš”', 'ì—¬ê¸°ì–´ë•Œ', 'ì•¼ë†€ì',\n","        'ë¡¯ë°', 'ì‹ ì„¸ê³„', 'í˜„ëŒ€', 'CJ', 'GS', 'SK', 'í•œí™”', 'ë‘ì‚°', 'í¬ìŠ¤ì½”', 'LG', 'ì‚¼ì„±',\n","        # ì¶”ê°€ì ìœ¼ë¡œ ì§€ì—­ë³„ íŠ¹ìˆ˜ í”„ëœì°¨ì´ì¦ˆë‚˜ ëŒ€ê¸°ì—… ì¶”ê°€ ê°€ëŠ¥\n","    ]\n","\n","    # ìƒí˜¸ëª… ì»¬ëŸ¼ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í–‰ì„ ì œì™¸í•©ë‹ˆë‹¤.\n","    pattern = '|'.join(franchise_keywords)\n","    # na=False: NaN ê°’ì€ í•„í„°ë§ì—ì„œ ì œì™¸ë˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬\n","    # ëŒ€ì†Œë¬¸ì ë¬´ì‹œë¥¼ ìœ„í•´ flags=re.IGNORECASE ì¶”ê°€\n","    filtered_df = df_input[~df_input['ìƒí˜¸ëª…'].str.contains(pattern, na=False, flags=re.IGNORECASE)].copy()\n","\n","    return filtered_df\n","\n","def analyze_morphemes(text):\n","    \"\"\"\n","    í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n","    \"\"\"\n","    if not isinstance(text, str) or not text.strip():\n","        return []\n","    morphemes = okt.nouns(text)\n","    return morphemes\n","\n","# 1. ë¶ˆìš©ì–´(Stopwords) ì •ì˜\n","# ë¶„ì„ ëª©ì ì— ë°©í•´ê°€ ë˜ëŠ” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n","# ìƒê¶Œ ë¶„ì„ì˜ ìˆœìˆ˜ì„±ì„ ìœ„í•´ ìƒí˜¸ëª…ì— ìì£¼ í¬í•¨ë˜ì§€ë§Œ, íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ë‹¨ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n","stopwords = {\n","    'ì ', 'ìƒí˜¸', 'í•˜ìš°ìŠ¤', 'ì§‘', 'ë³¸ì ', 'ì§€ì ', 'ì„¼í„°', 'ìŠ¤í† ì–´', 'ë§ˆì¼“',\n","    'ì•„íŒŒíŠ¸', 'ë¹Œë”©', 'íƒ€ìš´', 'ëŒ€ë¦¬ì ', 'ì‹œí‹°', 'ëœë“œ', 'ì›”ë“œ', 'íŒŒí¬', 'ê°€ë“ ', 'í”„ë¼ì', 'í´ëŸ½',\n","    'ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì‡¼í•‘', 'ëª°', 'ì•„ìš¸ë ›', 'ë”', 'ì•¤', 'ì•¤ë“œ', 'ì•¤', 'ë² ìŠ¤íŠ¸', 'ì„œë¹„ìŠ¤', 'ì „ë¬¸', 'ì œì¼',\n","    'ê·¸ë£¹', 'ì „', 'í›„', 'ì§€ì—ìŠ¤', 'ì”¨ìœ ', 'ì•ŒíŒŒ', 'ì˜¤í”¼ìŠ¤', 'ë„¥ìŠ¤', 'ì£¼ì‹íšŒì‚¬', 'ê¸°ì—…', 'ì½”ë¦¬ì•„',\n","    'í˜¸', 'ëª…', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ì™€', 'ë“±',\n","    # ë¶„ì„ ëŒ€ìƒ ì§€ì—­ëª…ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.\n","    # ì˜ˆ: 'íŒŒì£¼', 'ì„±ë‚¨', 'ê°€í‰' (ì´ë¯¸ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì§€ì—­ëª…ë„ ë¶ˆìš©ì–´ë¡œ ì¶”ê°€í•˜ì—¬ ì¼ë°˜ì ì¸ íŠ¹ì§• íŒŒì•…ì— ìœ ë¦¬í•©ë‹ˆë‹¤.)\n","    'ê°€í‰' # ê°€í‰êµ° ë°ì´í„° ë¶„ì„ ì‹œ 'ê°€í‰'ì´ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶ˆìš©ì–´ë¡œ ì¶”ê°€.\n","}\n","\n","def remove_stopwords(morpheme_list):\n","    \"\"\"\n","    í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ì˜ëœ ë¶ˆìš©ì–´ ë° í•œ ê¸€ì ë‹¨ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n","    \"\"\"\n","    # ë¶ˆìš©ì–´ ì œê±° ë° í•œ ê¸€ì ë‹¨ì–´ ì œì™¸\n","    filtered_list = [\n","        word for word in morpheme_list\n","        if word not in stopwords and len(word) > 1\n","    ]\n","    return filtered_list\n","\n","# ----------------------------------------------------------------------------\n","# 2. Identify CSV Files for Processing\n","# ----------------------------------------------------------------------------\n","# /content/ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ëª©ë¡ì„ ì°¾ìŠµë‹ˆë‹¤.\n","csv_files = glob.glob('/content/grouped_ìƒí˜¸ëª…_by_category/*.csv')\n","\n","if not csv_files:\n","    print(\"ğŸš¨ /content/ ë””ë ‰í† ë¦¬ì—ì„œ ì²˜ë¦¬í•  CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","else:\n","    print(f\"âœ… ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {', '.join([os.path.basename(f) for f in csv_files])}\\n\")\n","\n","    # ------------------------------------------------------------------------\n","    # 3. Process Each CSV File, Analyze Keywords, and Save Results\n","    # ------------------------------------------------------------------------\n","    for file_path in csv_files:\n","        file_name_base = os.path.basename(file_path).replace('.csv', '')\n","        print(f\"--- '{file_name_base}.csv' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ---\")\n","\n","        try:\n","            # ë°ì´í„° ë¡œë“œ\n","            df = pd.read_csv(file_path)\n","\n","            # 'ìƒí˜¸ëª…' ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°\n","            if 'ìƒí˜¸ëª…' not in df.columns:\n","                print(f\"âš ï¸ '{file_name_base}.csv' íŒŒì¼ì— 'ìƒí˜¸ëª…' ì»¬ëŸ¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.\")\n","                continue\n","\n","            # í…ìŠ¤íŠ¸ ì •ì œ ì ìš©\n","            df['cleaned_name'] = df['ìƒí˜¸ëª…'].apply(clean_business_name)\n","\n","            # í”„ëœì°¨ì´ì¦ˆ í•„í„°ë§ ì ìš©\n","            # ì›ë³¸ ë°ì´í„°ê°€ ë³€ê²½ë˜ì§€ ì•Šë„ë¡ .copy() ì‚¬ìš©\n","            df_filtered = filter_franchise(df.copy())\n","\n","            # í˜•íƒœì†Œ ë¶„ì„ ì ìš©\n","            df_filtered['morphemes'] = df_filtered['cleaned_name'].apply(analyze_morphemes)\n","\n","            # ë¶ˆìš©ì–´ ì œê±° ì ìš©\n","            df_filtered['final_keywords'] = df_filtered['morphemes'].apply(remove_stopwords)\n","\n","            # ëª¨ë“  ìµœì¢… í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°\n","            all_final_keywords = list(itertools.chain.from_iterable(df_filtered['final_keywords']))\n","\n","            # ë¹ˆë„ìˆ˜ ê³„ì‚°\n","            final_keyword_counts = Counter(all_final_keywords)\n","\n","            # ê°€ì¥ í”í•œ ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶œë ¥\n","            N = 30\n","            print(f\"\\n--- '{file_name_base}' íŒŒì¼ì˜ ìƒìœ„ {N}ê°œ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ ---\")\n","\n","            if final_keyword_counts:\n","                print(final_keyword_counts.most_common(N))\n","\n","                # ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSVë¡œ ì €ì¥\n","                df_final_counts = pd.DataFrame(final_keyword_counts.most_common(),\n","                                               columns=['Keyword', 'Frequency'])\n","                output_csv_path = f'{file_name_base}_final_keyword_frequency.csv'\n","                df_final_counts.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n","                print(f\"âœ… ìµœì¢… í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ '{output_csv_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","            else:\n","                print(\"âš ï¸ ë¶„ì„í•  ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n","\n","        except Exception as e:\n","            print(f\"âŒ '{file_name_base}.csv' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","        print(f\"\\n--- '{file_name_base}.csv' íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ---\\n\")\n","\n","print(\"--- ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ---\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ì—…ì¢…ë³„ ìƒí˜¸ëª… ì¶”ì¶œ ì½”ë“œ"],"metadata":{"id":"aaITYOB3dm3q"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import re\n","\n","# --- ì„¤ì • ë³€ìˆ˜ ---\n","# âš ï¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","FILE_PATH = 'ì§€ì—­í™”í ê°€ë§¹ì  í˜„í™©_20250801.csv'\n","# âš ï¸ ì»¬ëŸ¼ ì´ë¦„ ì„¤ì • (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ì¡°ì • í•„ìš”)\n","INDUSTRY_COL = 'ì—…ì¢…ëª…(ì¢…ëª©ëª…)' # ê·¸ë£¹í•‘ ê¸°ì¤€ ì»¬ëŸ¼\n","NAME_COL = 'ìƒí˜¸ëª…' # CSV íŒŒì¼ì— í¬í•¨í•  ìƒí˜¸ëª… ì»¬ëŸ¼\n","# âš ï¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n","OUTPUT_DIR = 'grouped_ìƒí˜¸ëª…_by_category'\n","\n","# --- ì¹´í…Œê³ ë¦¬ ì •ì˜ ---\n","# ì—…ì¢…ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ê·¸ë£¹í•‘í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ë° í‚¤ì›Œë“œ ì •ì˜\n","# ì—…ì¢…ëª…ì— ì•„ë˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.\n","CATEGORIES = {\n","    'ìŒì‹ì ': ['ì¼ë°˜ìŒì‹ì ', 'íœ´ê²ŒìŒì‹ì ', 'ë¶„ì‹', 'ë·”í˜', 'ì–‘ì‹', 'ì¼ì‹', 'ì¤‘ì‹', 'ì¹˜í‚¨', 'íŒ¨ìŠ¤íŠ¸í‘¸ë“œ'],\n","    'ìŒë£Œ/ë””ì €íŠ¸': ['ì»¤í”¼', 'ì œê³¼', 'ì•„ì´ìŠ¤í¬ë¦¼', 'ë–¡', 'ìŒë£Œ'],\n","    'ë§ˆíŠ¸/í¸ì˜ì /ìœ í†µ': ['ìŠˆí¼ë§ˆì¼“', 'í¸ì˜ì ', 'ë§ˆíŠ¸', 'ìœ í†µ', 'ì‹ìŒë£Œ'],\n","    'ë³‘ì›/ì•½êµ­/ì˜ë£Œ': ['ë³‘ì›', 'ì˜ì›', 'ì•½êµ­', 'í•œì˜ì›', 'ì¹˜ê³¼', 'ì˜ë£Œê¸°ê¸°', 'ì‚°í›„ì¡°ë¦¬ì›'],\n","    'ë¯¸ìš©/ë·°í‹°/ìœ„ìƒ': ['ë¯¸ìš©ì‹¤', 'ì´ë°œì†Œ', 'í”¼ë¶€', 'ë„¤ì¼', 'í™”ì¥í’ˆ', 'ëª©ìš•', 'ì„¸íƒ'],\n","    'í•™ì›/êµìœ¡': ['í•™ì›', 'êµìŠµ', 'êµìœ¡', 'ë…ì„œì‹¤'],\n","    'ë ˆì €/ìŠ¤í¬ì¸ ': ['ê³¨í”„', 'í—¬ìŠ¤', 'ìŠ¤í¬ì¸ ', 'ë ˆì €', 'ë…¸ë˜ë°©', 'ë‹¹êµ¬ì¥', 'ìˆ˜ì˜ì¥', 'ìŠ¤í‚¤'],\n","    'ìˆ™ë°•/ì—¬í–‰': ['ìˆ™ë°•', 'íœì…˜', 'ëª¨í…”', 'í˜¸í…”', 'ì—¬í–‰', 'ìº í•‘'],\n","    'ìë™ì°¨/ì£¼ìœ ': ['ì£¼ìœ ì†Œ', 'ì¶©ì „ì†Œ', 'ì •ë¹„', 'ì„¸ì°¨', 'íƒ€ì´ì–´', 'ë¶€í’ˆ', 'ê²¬ì¸'],\n","    'ê°€êµ¬/ì¸í…Œë¦¬ì–´/ê±´ì¶•': ['ê°€êµ¬', 'ì¸í…Œë¦¬ì–´', 'ì² ë¬¼', 'ê±´ì¶•', 'ì¡°ëª…', 'í˜ì¸íŠ¸'],\n","    'ì˜ë¥˜/ì¡í™”': ['ì˜ë¥˜', 'ì‹ ë°œ', 'ê°€ë°©', 'ì•ˆê²½', 'ì‹œê³„', 'ê·€ê¸ˆì†', 'ì•…ì„¸ì‚¬ë¦¬'],\n","    'ë†ì¶•ìˆ˜ì‚°ë¬¼': ['ì •ìœ¡', 'ì¶•ì‚°', 'ìˆ˜ì‚°', 'ë†ì‚°', 'ì²­ê³¼', 'ë¹„ë£Œ', 'ì‚¬ë£Œ'],\n","    'ë°˜ë ¤ë™ë¬¼': ['ë™ë¬¼ë³‘ì›', 'ì• ì™„'],\n","    'ê¸°íƒ€ ì„œë¹„ìŠ¤': ['ë¶€ë™ì‚°', 'ì„œë¹„ìŠ¤', 'ìˆ˜ë¦¬', 'ì‚¬ì§„', 'ì¸ì‡„', 'ê´‘ê³ ']\n","}\n","\n","# --- ë©”ì¸ í•¨ìˆ˜ ---\n","def group_and_export_merchants(file_path, industry_col, name_col, categories, output_dir):\n","    \"\"\"\n","    CSV íŒŒì¼ì„ ì½ì–´ì™€ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ìƒí˜¸ëª…ì„ ê·¸ë£¹í•‘í•˜ê³ ,\n","    ê° ê·¸ë£¹ë³„ë¡œ ìƒí˜¸ëª…ë§Œ í¬í•¨ëœ CSV íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n","    \"\"\"\n","\n","    # 1. ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€)\n","    df = pd.DataFrame()\n","    try:\n","        df = pd.read_csv(file_path, encoding='utf-8')\n","        print(f\"âœ… íŒŒì¼ì„ 'utf-8' ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.\")\n","    except UnicodeDecodeError:\n","        try:\n","            df = pd.read_csv(file_path, encoding='cp949')\n","            print(f\"âœ… íŒŒì¼ì„ 'cp949' ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.\")\n","        except Exception as e:\n","            print(f\"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (íŒŒì¼ ê²½ë¡œ: {file_path})\")\n","            print(f\"ìì„¸í•œ ì˜¤ë¥˜: {e}\")\n","            return\n","\n","    if df.empty:\n","        print(\"âŒ ì˜¤ë¥˜: ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆì–´ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    # 2. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n","    if industry_col not in df.columns or name_col not in df.columns:\n","        print(f\"âŒ ì˜¤ë¥˜: ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ ('{industry_col}' ë˜ëŠ” '{name_col}')ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","        print(f\"í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}\")\n","        return\n","\n","    # 3. ê·¸ë£¹í•‘ì„ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ìƒì„±\n","    print(\"â³ ì—…ì¢…ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ë§¹ì  ê·¸ë£¹í•‘ ì¤‘...\")\n","\n","    # 'Category' ì»¬ëŸ¼ì„ ì´ˆê¸°í™”í•˜ê³ , ë§¤ì¹­ë˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n","    df['Category'] = 'ê¸°íƒ€_ë¯¸ë¶„ë¥˜'\n","\n","    # ì—…ì¢…ëª… ë¬¸ìì—´ì„ ì†Œë¬¸ì/ê³µë°± ì œê±° ì²˜ë¦¬í•˜ì—¬ ë§¤ì¹­ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìœ¼ë‚˜,\n","    # ì—¬ê¸°ì„œëŠ” ì›ë³¸ ë¬¸ìì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","    for category, keywords in categories.items():\n","        # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì—…ì¢…ëª…ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ ìƒì„±\n","        pattern = r\"|\".join([re.escape(k) for k in keywords])\n","\n","        # 'Category' ì»¬ëŸ¼ì´ ì•„ì§ 'ê¸°íƒ€_ë¯¸ë¶„ë¥˜'ì¸ í–‰ì— ëŒ€í•´ì„œë§Œ ì²˜ë¦¬\n","        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë§¤ì¹­ë˜ëŠ” í–‰ì˜ 'Category' ê°’ì„ ì—…ë°ì´íŠ¸\n","        df.loc[\n","            (df['Category'] == 'ê¸°íƒ€_ë¯¸ë¶„ë¥˜') & df[industry_col].str.contains(pattern, case=False, na=False),\n","            'Category'\n","        ] = category\n","\n","    print(\"âœ… ê·¸ë£¹í•‘ ì™„ë£Œ.\")\n","\n","    # 4. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n","    os.makedirs(output_dir, exist_ok=True)\n","    print(f\"ì €ì¥ ë””ë ‰í† ë¦¬: '{output_dir}'\")\n","\n","    # 5. ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒí˜¸ëª…ë§Œ ì¶”ì¶œí•˜ì—¬ CSV íŒŒì¼ ìƒì„±\n","    grouped = df.groupby('Category')\n","    file_count = 0\n","\n","    print(\"â³ ê·¸ë£¹ë³„ ìƒí˜¸ëª… CSV íŒŒì¼ ìƒì„± ì¤‘...\")\n","    for category_name, group_data in grouped:\n","        # íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ë¬¸ìë¥¼ ì œê±°í•˜ê³  ê³µë°±ì„ ë°‘ì¤„(_)ë¡œ ëŒ€ì²´\n","        safe_group_name = re.sub(r'[\\\\/:*?\"<>|]', '', category_name).replace(' ', '_').strip('_')\n","\n","        if not safe_group_name:\n","            safe_group_name = \"ë¯¸ë¶„ë¥˜_ê·¸ë£¹\"\n","\n","        file_name = f\"{safe_group_name}.csv\"\n","        output_path = os.path.join(output_dir, file_name)\n","\n","        # ìƒí˜¸ëª… ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³ , ì»¬ëŸ¼ ì´ë¦„ì„ 'ìƒí˜¸ëª…'ìœ¼ë¡œ ê³ ì •\n","        export_df = group_data[[name_col]].rename(columns={name_col: 'ìƒí˜¸ëª…'})\n","\n","        # CSV íŒŒì¼ë¡œ ì €ì¥ (ì¸ë±ìŠ¤ ì œì™¸)\n","        export_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n","\n","        file_count += 1\n","\n","    print(\"---\")\n","    print(f\"ğŸ‰ ì´ {file_count}ê°œì˜ ì¹´í…Œê³ ë¦¬ë³„ CSV íŒŒì¼ì´ '{output_dir}'ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","    print(\"ìƒì„±ëœ íŒŒì¼ì—ëŠ” ê° ê·¸ë£¹ì— ì†í•˜ëŠ” ê°€ë§¹ì ì˜ ìƒí˜¸ëª…ë§Œ í¬í•¨ë©ë‹ˆë‹¤.\")\n","\n","# --- ì½”ë“œ ì‹¤í–‰ ---\n","group_and_export_merchants(FILE_PATH, INDUSTRY_COL, NAME_COL, CATEGORIES, OUTPUT_DIR)"],"metadata":{"id":"31I_OuwRi8aF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["êµ°ì§‘ë¶„ì„ ì½”ë“œ"],"metadata":{"id":"_fEsvdhydW_l"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import numpy as np\n","import json # ë”•ì…”ë„ˆë¦¬ ì¶œë ¥ì„ ìœ„í•´ ì¶”ê°€\n","from konlpy.tag import Okt\n","from collections import Counter\n","import itertools\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","\n","# ----------------------------------------------------------------------------\n","# 1. ì„¤ì • ë³€ìˆ˜\n","# ----------------------------------------------------------------------------\n","FILE_PATH = '/content/grouped_ìƒí˜¸ëª…_by_category/í•™ì›êµìœ¡.csv' # âš ï¸ ì—¬ê¸°ì— ì‹¤ì œ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n","BUSINESS_NAME_COLUMN = 'ìƒí˜¸ëª…'# âš ï¸ ìƒí˜¸ëª…ì´ í¬í•¨ëœ ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ í›„ ìˆ˜ì •\n","K_CLUSTERS = 5 # ë¶„ì„ì— ì‚¬ìš©í•  êµ°ì§‘(í´ëŸ¬ìŠ¤í„°) ê°œìˆ˜\n","\n","# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n","okt = Okt()\n","\n","# ----------------------------------------------------------------------------\n","# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n","# ----------------------------------------------------------------------------\n","\n","def clean_business_name(text):\n","    \"\"\" ìƒí˜¸ëª… í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë° ìˆ˜ì‹ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤. \"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = re.sub(r'\\([^)]*\\)', '', text)\n","    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n","    text = re.sub(r'\\d+í˜¸?ì ', '', text)\n","    text = re.sub(r'\\s\\w+ì \\b', '', text)\n","    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","def filter_franchise(df_input, col_name='ìƒí˜¸ëª…'):\n","    \"\"\" ëŒ€ê·œëª¨ ì²´ì¸ì  ìƒí˜¸ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. \"\"\"\n","    franchise_keywords = [\n","        'ìŠ¤íƒ€ë²…ìŠ¤', 'ì§€ì—ìŠ¤', 'ì”¨ìœ ', 'ì„¸ë¸ì¼ë ˆë¸', 'íŒŒë¦¬ë°”ê²Œëœ¨', 'ë§¥ë„ë‚ ë“œ', 'ë¡¯ë°ë§ˆíŠ¸',\n","        'ë²„ê±°í‚¹', 'KFC', 'ì´ë§ˆíŠ¸', 'í™ˆí”ŒëŸ¬ìŠ¤', 'ë‹¤ì´ì†Œ', 'ì˜¬ë¦¬ë¸Œì˜', 'ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤',\n","        'ë˜í‚¨ë„ë„ˆì¸ ', 'íˆ¬ì¸í”Œë ˆì´ìŠ¤', 'ë¹½ë‹¤ë°©', 'ì´ë””ì•¼', 'ë©”ê°€ì»¤í”¼', 'ë§˜ìŠ¤í„°ì¹˜', 'BBQ',\n","        'êµ­ë¯¼ì€í–‰', 'ì‹ í•œì€í–‰', 'ì‚¼ì„±', 'LG', 'SK', 'CJ','ì´ë§ˆíŠ¸'\n","    ]\n","    pattern = '|'.join(franchise_keywords)\n","    # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ (flags=re.IGNORECASE)\n","    filtered_df = df_input[~df_input[col_name].str.contains(pattern, na=False, flags=re.IGNORECASE)].copy()\n","    return filtered_df\n","\n","def analyze_morphemes(text):\n","    \"\"\" í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤. \"\"\"\n","    if not isinstance(text, str) or not text.strip():\n","        return []\n","    return okt.nouns(text)\n","\n","# ë¶ˆìš©ì–´ ì •ì˜ (ë¶„ì„ ëª©ì ì— ë§ê²Œ ì¶”ê°€/ì‚­ì œ ê°€ëŠ¥)\n","stopwords = {\n","    'ì ', 'ìƒí˜¸', 'í•˜ìš°ìŠ¤', 'ì§‘', 'ë³¸ì ', 'ì§€ì ', 'ì„¼í„°', 'ìŠ¤í† ì–´', 'ë§ˆì¼“',\n","    'ì•„íŒŒíŠ¸', 'ë¹Œë”©', 'íƒ€ìš´', 'ì‹œí‹°', 'ëœë“œ', 'ì›”ë“œ', 'íŒŒí¬', 'ê°€ë“ ', 'í”„ë¼ì', 'í´ëŸ½',\n","    'ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì‡¼í•‘', 'ëª°', 'ì•„ìš¸ë ›', 'ë”', 'ì•¤', 'ë² ìŠ¤íŠ¸', 'ì„œë¹„ìŠ¤', 'ì „ë¬¸', 'ì œì¼',\n","    'ê·¸ë£¹', 'ì „', 'í›„', 'í˜¸', 'ëª…', 'ì´', 'ê°€', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼',\n","    'ë„', 'ë§Œ', 'ë¡œ', 'ë“±',\n","    # âš ï¸ ë¶„ì„ ëŒ€ìƒ ì§€ì—­ëª…ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”.\n","}\n","\n","def remove_stopwords(morpheme_list):\n","    \"\"\" í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ì˜ëœ ë¶ˆìš©ì–´ ë° í•œ ê¸€ì ë‹¨ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤. \"\"\"\n","    filtered_list = [\n","        word for word in morpheme_list\n","        if word not in stopwords and len(word) > 1\n","    ]\n","    return filtered_list\n","\n","# ----------------------------------------------------------------------------\n","# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n","# ----------------------------------------------------------------------------\n","\n","try:\n","    # 1) ë°ì´í„° ë¡œë“œ\n","    df = pd.read_csv(FILE_PATH)\n","\n","    if BUSINESS_NAME_COLUMN not in df.columns:\n","        print(f\"ğŸš¨ ì§€ì •ëœ ì»¬ëŸ¼ëª… ('{BUSINESS_NAME_COLUMN}')ì´ íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.\")\n","        exit()\n","\n","    # 2) í…ìŠ¤íŠ¸ ì •ì œ ë° í”„ëœì°¨ì´ì¦ˆ í•„í„°ë§\n","    df['cleaned_name'] = df[BUSINESS_NAME_COLUMN].apply(clean_business_name)\n","    df_filtered = filter_franchise(df, col_name='cleaned_name')\n","\n","    # 3) í˜•íƒœì†Œ ë¶„ì„ ë° ë¶ˆìš©ì–´ ì œê±°\n","    df_filtered['morphemes'] = df_filtered['cleaned_name'].apply(analyze_morphemes)\n","    df_filtered['final_keywords'] = df_filtered['morphemes'].apply(remove_stopwords)\n","\n","    # 4) êµ°ì§‘ë¶„ì„ìš© ë¬¸ìì—´ ìƒì„± (keyword_string)\n","    df_filtered['keyword_string'] = df_filtered['final_keywords'].apply(lambda x: ' '.join(x))\n","\n","    # ìœ íš¨ ë°ì´í„° í™•ì¸\n","    df_analysis = df_filtered[df_filtered['keyword_string'].str.strip() != '']\n","    if df_analysis.empty:\n","        print(\"âš ï¸ ëª¨ë“  ì „ì²˜ë¦¬ í›„ ë¶„ì„í•  ìœ íš¨í•œ ìƒí˜¸ëª… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n","        exit()\n","\n","    print(f\"âœ… ì „ì²˜ë¦¬ ë° í•„í„°ë§ ì™„ë£Œ. ì›ë³¸ ë°ì´í„° {len(df)}ê°œ ì¤‘ ìµœì¢… ë¶„ì„ ë°ì´í„° {len(df_analysis)}ê°œ.\")\n","\n","    # ------------------------------------------------------------------------\n","    # 4. êµ°ì§‘ë¶„ì„ (Clustering Analysis)\n","    # ------------------------------------------------------------------------\n","\n","    # 1) í‚¤ì›Œë“œ ë²¡í„°í™” (TF-IDF)\n","    tfidf = TfidfVectorizer(min_df=5) # ìµœì†Œ 5ê°œ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œë§Œ ì‚¬ìš©\n","    tfidf_matrix = tfidf.fit_transform(df_analysis['keyword_string'])\n","    feature_names = tfidf.get_feature_names_out()\n","\n","    print(f\"\\nâœ… TF-IDF í–‰ë ¬ ìƒì„± ì™„ë£Œ. Shape: {tfidf_matrix.shape}\")\n","\n","    # 2) K-Means êµ°ì§‘ë¶„ì„ ì ìš©\n","    actual_k = min(K_CLUSTERS, tfidf_matrix.shape[0])\n","    if actual_k < 2:\n","        print(\"âš ï¸ êµ°ì§‘ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸°ì— ìœ íš¨ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (2ê°œ ë¯¸ë§Œ).\")\n","    else:\n","        kmeans = KMeans(n_clusters=actual_k, random_state=42, n_init=10)\n","        kmeans.fit(tfidf_matrix)\n","        df_analysis['cluster'] = kmeans.labels_\n","\n","        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— êµ°ì§‘ ê²°ê³¼ ë³‘í•©\n","        df_filtered = df_filtered.merge(df_analysis[['cluster']], left_index=True, right_index=True, how='left')\n","        df_filtered['cluster'] = df_filtered['cluster'].fillna(-1).astype(int) # êµ°ì§‘ ë¯¸í¬í•¨ ë°ì´í„°ëŠ” -1ë¡œ í‘œì‹œ\n","\n","        print(f\"\\nâœ… K-Means êµ°ì§‘ë¶„ì„ ì™„ë£Œ (K={actual_k})\")\n","        print(f\"êµ°ì§‘ë³„ ë°ì´í„° ê°œìˆ˜:\\n{df_analysis['cluster'].value_counts().sort_index()}\")\n","\n","        # 3) êµ°ì§‘ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ë¶„ì„ ë° ì‹œê°í™”ìš© ë”•ì…”ë„ˆë¦¬ ì¶œë ¥ (ìˆ˜ì •)\n","\n","        def get_top_keywords(cluster_index, top_n=10):\n","            \"\"\" êµ°ì§‘ë³„ ìƒìœ„ í‚¤ì›Œë“œì™€ TF-IDF ì ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. \"\"\"\n","            centroid = kmeans.cluster_centers_\n","            sorted_centroid_indices = centroid[cluster_index].argsort()[::-1]\n","            top_n_indices = sorted_centroid_indices[:top_n]\n","            keywords = [feature_names[i] for i in top_n_indices]\n","            scores = [round(centroid[cluster_index][i], 3) for i in top_n_indices]\n","            return list(zip(keywords, scores))\n","\n","        top_keywords_by_cluster = {}\n","        print(\"\\n--- êµ°ì§‘ë³„ ìƒìœ„ íŠ¹ì§• í‚¤ì›Œë“œ ë° ì‹œê°í™”ìš© ë”•ì…”ë„ˆë¦¬ ---\")\n","\n","        for i in range(actual_k):\n","            # ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ of íŠœí”Œ í˜•íƒœ)\n","            top_keywords_list = get_top_keywords(i)\n","\n","            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜: {'í‚¤ì›Œë“œ': ì ìˆ˜, ...}\n","            keywords_dict = {keyword: score for keyword, score in top_keywords_list}\n","            top_keywords_by_cluster[i] = keywords_dict\n","\n","            cluster_size = len(df_analysis[df_analysis['cluster'] == i])\n","            # ê¸°ì¡´ ì¶œë ¥ í˜•ì‹ë„ ìœ ì§€í•˜ì—¬ ì¤‘ê°„ í™•ì¸ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n","            print(f\"**Cluster {i} (ë°ì´í„° ìˆ˜: {cluster_size}ê°œ):**\")\n","            print(top_keywords_list)\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸš¨ ì•„ë˜ 'top_keywords_for_visualization' ë³€ìˆ˜ì˜ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ì‹œê°í™” ì½”ë“œì— ë°”ë¡œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.\")\n","\n","        # ìµœì¢… ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ê³  JSON í˜•íƒœë¡œ ì¶œë ¥\n","        visualization_dict_output = json.dumps(top_keywords_by_cluster, ensure_ascii=False, indent=4)\n","        print(\"top_keywords_for_visualization = \\\\\")\n","        print(visualization_dict_output)\n","\n","        print(\"=\" * 60)\n","\n","        # 4) ìµœì¢… ê²°ê³¼ ì €ì¥\n","        output_data_path = 'final_clustered_business_data.csv'\n","        df_filtered.to_csv(output_data_path, index=False, encoding='utf-8-sig')\n","        print(f\"\\nâœ… êµ°ì§‘ë¶„ì„ ê²°ê³¼ê°€ í¬í•¨ëœ ìµœì¢… ë°ì´í„°ê°€ '{output_data_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","except FileNotFoundError:\n","    print(f\"ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{FILE_PATH}'. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n","except Exception as e:\n","    print(f\"âŒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}\")"],"metadata":{"id":"KIcG4Iae5wwW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["êµ°ì§‘ë¶„ì„ ì‹œê°í™” ì½”ë“œ"],"metadata":{"id":"Xjb2zEMIdBDS"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# â˜ â€‚ì´ ì½”ë“œëŠ” 'final_clustered_business_data.csv'ì™€ 'final_keyword_frequency.csv' íŒŒì¼ì´\n","#    ë³„ë„ë¡œ ì¡´ì¬í•˜ê±°ë‚˜, ì´ì „ ë‹¨ê³„ì˜ êµ°ì§‘ íŠ¹ì§• í‚¤ì›Œë“œ ê²°ê³¼ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n","#    ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ êµ°ì§‘ íŠ¹ì§• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n","\n","# --- (ì˜ˆì‹œ ë°ì´í„°: ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë¡œ ëŒ€ì²´í•´ì•¼ í•¨) ---\n","# ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ìƒìœ„ 10ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ì‹œê°í™” ì‹œ ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","top_keywords_by_cluster = {\n","   \"0\": {\n","        \"ìŒì•…í•™\": 0.585,\n","        \"í”¼ì•„ë…¸\": 0.045,\n","        \"ì´í™”\": 0.024,\n","        \"ë®¤ì§\": 0.024,\n","        \"ì†Œë¦¬\": 0.021,\n","        \"ë®¤ì¦ˆ\": 0.014,\n","        \"ìˆ™ëª…\": 0.011,\n","        \"í´ë‘\": 0.009,\n","        \"ì¹¸íƒ€ë¹Œë ˆ\": 0.008,\n","        \"ë©œë¡œë””\": 0.008\n","    },\n","    \"1\": {\n","        \"êµìŠµ\": 0.335,\n","        \"ìˆ˜í•™\": 0.263,\n","        \"ë¯¸ìˆ \": 0.115,\n","        \"ìŒì•…\": 0.097,\n","        \"í”¼ì•„ë…¸\": 0.08,\n","        \"ì•„íŠ¸\": 0.036,\n","        \"êµ­ì–´\": 0.017,\n","        \"ë…¼ìˆ \": 0.014,\n","        \"ê·¸ë¦¼\": 0.013,\n","        \"í•´ë²•\": 0.007\n","    },\n","    \"2\": {\n","        \"ì˜ì–´\": 0.491,\n","        \"êµìŠµ\": 0.178,\n","        \"ë®¤ì— \": 0.053,\n","        \"í•™ì›\": 0.051,\n","        \"ìˆ˜í•™í•™ì›\": 0.031,\n","        \"ìˆ˜í•™\": 0.025,\n","        \"ì‰ê¸€ë¦¬ì‹œ\": 0.021,\n","        \"ì‰ê¸€ë¦¬ì‰¬\": 0.021,\n","        \"ë³´ìŠµ\": 0.019,\n","        \"ë¯¸ë˜ì—”\": 0.016\n","    },\n","    \"3\": {\n","        \"í•™ì›\": 0.378,\n","        \"ë¯¸ìˆ \": 0.075,\n","        \"ë³´ìŠµ\": 0.049,\n","        \"ìˆ˜í•™\": 0.037,\n","        \"ì‹¤ìš©ìŒì•…\": 0.026,\n","        \"ì•„íŠ¸\": 0.023,\n","        \"ì—ë“€\": 0.021,\n","        \"í”¼ì•„ë…¸\": 0.016,\n","        \"ì‰ê¸€ë¦¬ì‰¬\": 0.015,\n","        \"ì…ì‹œ\": 0.014\n","    },\n","    \"4\": {\n","        \"ìˆ˜í•™í•™ì›\": 0.051,\n","        \"íƒœê¶Œë„\": 0.045,\n","        \"ì˜ì–´í•™ì›\": 0.043,\n","        \"ìŠ¤í„°ë””\": 0.025,\n","        \"ì¹´í˜\": 0.02,\n","        \"ë…¼ìˆ \": 0.016,\n","        \"í•™ì›\": 0.015,\n","        \"ë¶€ë°©\": 0.015,\n","        \"êµìœ¡\": 0.014,\n","        \"ë…ì„œ\": 0.013\n","    }\n","}\n","# --- (ì˜ˆì‹œ ë°ì´í„° ë) ---\n","\n","# ì‹œê°í™” ì„¤ì •\n","TOP_N_KEYWORDS = 10\n","\n","for cluster, keywords in top_keywords_by_cluster.items():\n","    # 1. í‚¤ì›Œë“œë¥¼ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ì„ íƒ\n","    sorted_keywords = sorted(keywords.items(), key=lambda item: item[1], reverse=True)[:TOP_N_KEYWORDS]\n","\n","    # 2. DataFrame ìƒì„±\n","    keywords_df = pd.DataFrame(sorted_keywords, columns=['Keyword', 'Score'])\n","\n","    # ìƒˆë¡œìš´ Figureì™€ Axes ìƒì„± (ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë³„ë„ì˜ ê·¸ë˜í”„)\n","    fig, ax = plt.subplots(figsize=(6, 5)) # ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •\n","\n","    # 3. ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ê°€ë¡œ ë§‰ëŒ€ ê·¸ë˜í”„)\n","    sns.barplot(\n","        x='Score',  # xì¶•ì„ ì ìˆ˜ë¡œ ì„¤ì •\n","        y='Keyword',  # yì¶•ì„ í‚¤ì›Œë“œë¡œ ì„¤ì •\n","        data=keywords_df,\n","        ax=ax,\n","        palette='viridis',\n","        hue='Keyword', # 'y' ë³€ìˆ˜ë¥¼ hueë¡œ ì„¤ì •í•˜ì—¬ FutureWarning í•´ê²°\n","        legend=False # ë²”ë¡€ ë¹„í™œì„±í™”\n","    )\n","\n","    ax.set_title(f'Cluster {cluster} ìƒìœ„ {TOP_N_KEYWORDS}ê°œ íŠ¹ì§• í‚¤ì›Œë“œ', fontsize=14) # ê° ê·¸ë˜í”„ì— ì œëª© ì„¤ì •\n","    ax.set_xlabel('TF-IDF Score', fontsize=12)\n","    ax.set_ylabel('') # yì¶• ë¼ë²¨ ì´ë¦„ ì œê±° (í‚¤ì›Œë“œ ìì²´ê°€ ë¼ë²¨ì´ë¯€ë¡œ)\n","\n","    plt.tight_layout() # ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì¡°ì •\n","    plt.show() # ê° ê·¸ë˜í”„ë¥¼ ê°œë³„ì ìœ¼ë¡œ í‘œì‹œ"],"outputs":[],"execution_count":null,"metadata":{"id":"n857YKVv_6aP"}}]}